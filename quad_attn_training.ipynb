{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Using EXACT Garg et al. transformer implementation\n",
        "Simple training loop for your data format\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Model, GPT2Config\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "# ============================================================================\n",
        "# EXACT GARG ET AL. TRANSFORMER (from their repo)\n",
        "# ============================================================================\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, n_dims, n_positions, n_embd=128, n_layer=12, n_head=4):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        configuration = GPT2Config(\n",
        "            n_positions=2 * n_positions,\n",
        "            n_embd=n_embd,\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            resid_pdrop=0.0,\n",
        "            embd_pdrop=0.0,\n",
        "            attn_pdrop=0.0,\n",
        "            use_cache=False,\n",
        "        )\n",
        "        self.name = f\"gpt2_embd={n_embd}_layer={n_layer}_head={n_head}\"\n",
        "\n",
        "        self.n_positions = n_positions\n",
        "        self.n_dims = n_dims\n",
        "        self._read_in = nn.Linear(n_dims, n_embd)\n",
        "        self._backbone = GPT2Model(configuration)\n",
        "        self._read_out = nn.Linear(n_embd, 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _combine(xs_b, ys_b):\n",
        "        \"\"\"Interleaves the x's and the y's into a single sequence.\"\"\"\n",
        "        bsize, points, dim = xs_b.shape\n",
        "        ys_b_wide = torch.cat(\n",
        "            (\n",
        "                ys_b.view(bsize, points, 1),\n",
        "                torch.zeros(bsize, points, dim - 1, device=ys_b.device),\n",
        "            ),\n",
        "            axis=2,\n",
        "        )\n",
        "        zs = torch.stack((xs_b, ys_b_wide), dim=2)\n",
        "        zs = zs.view(bsize, 2 * points, dim)\n",
        "        return zs\n",
        "\n",
        "    def forward(self, xs, ys, inds=None):\n",
        "        if inds is None:\n",
        "            inds = torch.arange(ys.shape[1])\n",
        "        else:\n",
        "            inds = torch.tensor(inds)\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "        zs = self._combine(xs, ys)\n",
        "        embeds = self._read_in(zs)\n",
        "        output = self._backbone(inputs_embeds=embeds).last_hidden_state\n",
        "        prediction = self._read_out(output)\n",
        "        return prediction[:, ::2, 0][:, inds]  # predict only on xs\n",
        "\n",
        "# ============================================================================\n",
        "# YOUR DATA LOADER\n",
        "# ============================================================================\n",
        "\n",
        "def load_data(data_path):\n",
        "    \"\"\"Load your isotropic_data.npz\"\"\"\n",
        "    data = np.load(data_path)\n",
        "\n",
        "    # Your format:\n",
        "    # X_train: (16000, 10, 5)\n",
        "    # y_train: (16000, 10)\n",
        "    # X_test: (4000, 10, 5)\n",
        "    # y_test: (4000, 10)\n",
        "\n",
        "    train_xs = torch.from_numpy(data['X_train']).float()\n",
        "    train_ys = torch.from_numpy(data['y_train']).float()\n",
        "    test_xs = torch.from_numpy(data['X_test']).float()\n",
        "    test_ys = torch.from_numpy(data['y_test']).float()\n",
        "\n",
        "    return train_xs, train_ys, test_xs, test_ys\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def train_step(model, xs, ys, optimizer, device):\n",
        "    \"\"\"Single training step\"\"\"\n",
        "    xs = xs.to(device)\n",
        "    ys = ys.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Model predicts at all positions\n",
        "    predictions = model(xs, ys)\n",
        "\n",
        "    # MSE loss\n",
        "    loss = ((predictions - ys) ** 2).mean()\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def eval_model(model, xs, ys, device, batch_size=64):\n",
        "    \"\"\"Evaluate model on dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(xs), batch_size):\n",
        "            batch_xs = xs[i:i+batch_size].to(device)\n",
        "            batch_ys = ys[i:i+batch_size].to(device)\n",
        "\n",
        "            predictions = model(batch_xs, batch_ys)\n",
        "            loss = ((predictions - batch_ys) ** 2).mean()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "    model.train()\n",
        "    return total_loss / n_batches\n",
        "\n",
        "def train_model(model, train_xs, train_ys, test_xs, test_ys, config, device):\n",
        "    \"\"\"Main training loop\"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
        "\n",
        "    n_train = len(train_xs)\n",
        "    best_test_loss = float('inf')\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'step': [],\n",
        "        'wall_time': []\n",
        "    }\n",
        "\n",
        "    start_time = time.time()\n",
        "    step = 0\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training for {config['steps']} steps\")\n",
        "    print(f\"Batch size: {config['batch_size']}\")\n",
        "    print(f\"Train samples: {n_train}, Test samples: {len(test_xs)}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    pbar = tqdm(total=config['steps'], desc=\"Training\")\n",
        "\n",
        "    while step < config['steps']:\n",
        "        # Sample random batch\n",
        "        batch_idx = torch.randint(0, n_train, (config['batch_size'],))\n",
        "        batch_xs = train_xs[batch_idx]\n",
        "        batch_ys = train_ys[batch_idx]\n",
        "\n",
        "        # Train step\n",
        "        train_loss = train_step(model, batch_xs, batch_ys, optimizer, device)\n",
        "\n",
        "        step += 1\n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({'loss': f'{train_loss:.4f}'})\n",
        "\n",
        "        # Log every 1000 steps\n",
        "        if step % 1000 == 0 or step == config['steps']:\n",
        "            test_loss = eval_model(model, test_xs, test_ys, device, config['batch_size'])\n",
        "            elapsed_time = time.time() - start_time\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['test_loss'].append(test_loss)\n",
        "            history['step'].append(step)\n",
        "            history['wall_time'].append(elapsed_time)\n",
        "\n",
        "            print(f\"\\nStep {step:5d} | Train Loss: {train_loss:.6f} | \"\n",
        "                  f\"Test Loss: {test_loss:.6f} | Time: {elapsed_time/3600:.2f}h\")\n",
        "\n",
        "            # Save best model\n",
        "            if test_loss < best_test_loss:\n",
        "                best_test_loss = test_loss\n",
        "                torch.save({\n",
        "                    'step': step,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'test_loss': test_loss,\n",
        "                    'wall_time': elapsed_time,\n",
        "                }, config['checkpoint_path'])\n",
        "                print(f\"✓ Saved new best model (test_loss: {test_loss:.6f})\")\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training complete!\")\n",
        "    print(f\"Best test loss: {best_test_loss:.6f}\")\n",
        "    print(f\"Total wall-clock time: {total_time/3600:.2f} hours\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set seed\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Config\n",
        "    config = {\n",
        "        # Model params (6-layer as you specified)\n",
        "        'n_dims': 5,         # your feature dimension\n",
        "        'n_positions': 10,   # your context points\n",
        "        'n_embd': 256,\n",
        "        'n_layer': 6,        # YOUR DEPTH\n",
        "        'n_head': 4,\n",
        "\n",
        "        # Training params\n",
        "        'batch_size': 32,\n",
        "        'lr': 1e-4,          # Garg et al. use 1e-4\n",
        "        'steps': 50000,      # 50k steps as in your proposal\n",
        "\n",
        "        # Paths\n",
        "        'data_path': 'data/isotropic_data.npz',\n",
        "        'checkpoint_path': 'checkpoints/transformer_6layer_best.pt',\n",
        "    }\n",
        "\n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    # Load your data\n",
        "    print(\"Loading data...\")\n",
        "    train_xs, train_ys, test_xs, test_ys = load_data(config['data_path'])\n",
        "    print(f\"Train: {train_xs.shape}, Test: {test_xs.shape}\")\n",
        "\n",
        "    # Create model (EXACT Garg et al. architecture)\n",
        "    print(\"\\nInitializing transformer model...\")\n",
        "    model = TransformerModel(\n",
        "        n_dims=config['n_dims'],\n",
        "        n_positions=config['n_positions'],\n",
        "        n_embd=config['n_embd'],\n",
        "        n_layer=config['n_layer'],\n",
        "        n_head=config['n_head']\n",
        "    )\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model: {model.name}\")\n",
        "    print(f\"Total parameters: {n_params:,}\")\n",
        "\n",
        "    # Train\n",
        "    print(\"\\nStarting training...\")\n",
        "    history = train_model(\n",
        "        model,\n",
        "        train_xs, train_ys,\n",
        "        test_xs, test_ys,\n",
        "        config,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Save history\n",
        "    history_path = 'results/training_history_6layer.json'\n",
        "    with open(history_path, 'w') as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "    print(f\"\\nHistory saved to {history_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9FjNmMz_HN_",
        "outputId": "4a3da7f6-811b-49d3-bc1a-60f0f28ce77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Loading data...\n",
            "Train: torch.Size([16000, 10, 5]), Test: torch.Size([4000, 10, 5])\n",
            "\n",
            "Initializing transformer model...\n",
            "Model: gpt2_embd=256_layer=6_head=4\n",
            "Total parameters: 17,611,777\n",
            "\n",
            "Starting training...\n",
            "\n",
            "======================================================================\n",
            "Training for 50000 steps\n",
            "Batch size: 32\n",
            "Train samples: 16000, Test samples: 4000\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 981/50000 [00:25<19:07, 42.71it/s, loss=5.1008]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_history(history_path, save_dir=\"results\"):\n",
        "    # Load history JSON\n",
        "    with open(history_path, \"r\") as f:\n",
        "        history = json.load(f)\n",
        "\n",
        "    train_loss = np.array(history[\"train_loss\"])\n",
        "    test_loss = np.array(history[\"test_loss\"])\n",
        "    steps = np.array(history[\"step\"])\n",
        "    wall_time = np.array(history[\"wall_time\"])  # seconds\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # 1) Loss vs training steps\n",
        "    plt.figure()\n",
        "    plt.plot(steps, train_loss, label=\"Train loss\")\n",
        "    plt.plot(steps, test_loss, label=\"Test loss\")\n",
        "    plt.xlabel(\"Training step\")\n",
        "    plt.ylabel(\"MSE loss\")\n",
        "    plt.title(\"Train/Test Loss vs Steps\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    loss_step_path = os.path.join(save_dir, \"loss_vs_steps.png\")\n",
        "    plt.savefig(loss_step_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"Saved {loss_step_path}\")\n",
        "\n",
        "    # 2) Loss vs wall-clock time (in hours)\n",
        "    wall_time_hours = wall_time / 3600.0\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(wall_time_hours, train_loss, label=\"Train loss\")\n",
        "    plt.plot(wall_time_hours, test_loss, label=\"Test loss\")\n",
        "    plt.xlabel(\"Wall-clock time (hours)\")\n",
        "    plt.ylabel(\"MSE loss\")\n",
        "    plt.title(\"Train/Test Loss vs Wall-Clock Time\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    loss_time_path = os.path.join(save_dir, \"loss_vs_time.png\")\n",
        "    plt.savefig(loss_time_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"Saved {loss_time_path}\")\n",
        "\n",
        "    # 3) Time per step (seconds/step) vs step\n",
        "    #   Δtime between checkpoints / Δsteps\n",
        "    dt = np.diff(wall_time)         # seconds between logs\n",
        "    dsteps = np.diff(steps)         # steps between logs (should be constant, e.g. 1000)\n",
        "    time_per_step = dt / dsteps     # seconds per step\n",
        "    step_centers = steps[1:]        # align each dt with the later step\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(step_centers, time_per_step)\n",
        "    plt.xlabel(\"Training step\")\n",
        "    plt.ylabel(\"Seconds per step\")\n",
        "    plt.title(\"Time per Step vs Training Step\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    tps_path = os.path.join(save_dir, \"time_per_step_vs_steps.png\")\n",
        "    plt.savefig(tps_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"Saved {tps_path}\")\n",
        "\n",
        "    # Optionally print summary stats\n",
        "    print(f\"Mean seconds/step: {time_per_step.mean():.6f}\")\n",
        "    print(f\"Std seconds/step: {time_per_step.std():.6f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    history_path = \"results/training_history_6layer.json\"  # adjust if needed\n",
        "    plot_history(history_path, save_dir=\"results\")"
      ],
      "metadata": {
        "id": "i-cCbXFF_abG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0KyXjaE-7E0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}